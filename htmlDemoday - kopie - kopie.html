<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Webcam Controlled Motor</title>
    <style>
        body {
            display: flex;
            flex-direction: column;
            align-items: center;
            font-family: Arial, sans-serif;
        }
        #container {
            display: flex;
            justify-content: space-around;
            align-items: flex-start;
            width: 100%;
            max-width: 1200px;
        }
        #webcamContainer, #videoContainer {
            position: relative;
            width: 48%;
        }
        #webcamVideo, #mainVideo {
            width: 100%;
            height: auto;
        }
        #canvas {
            position: absolute;
            top: 0;
            left: 0;
        }
        header {
            width: 100%;
            text-align: center;
            padding: 10px 0;
            background-color: #333;
            color: white;
            font-size: 24px;
        }
    </style>
</head>
<body>
    <div id="container">
        <div id="webcamContainer">
            <video id="webcamVideo" autoplay></video>
            <canvas id="canvas"></canvas>
        </div>
        <div id="videoContainer">
            <video id="mainVideo" controls>
                <source src="Videos/ezgif.com-video-speed.mp4" type="video/mp4">
            </video>
        </div>
    </div>
    <script defer src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/@vladmandic/face-api"></script>
    <script>
        const video = document.getElementById('webcamVideo');
        const canvas = document.getElementById('canvas');
        const context = canvas.getContext('2d');
        let isPlaying = false;

        async function setupCamera() {
            const stream = await navigator.mediaDevices.getUserMedia({ video: {} });
            video.srcObject = stream;
            return new Promise((resolve) => {
                video.onloadedmetadata = () => {
                    resolve(video);
                };
            });
        }

        async function loadModels() {
            await faceapi.nets.tinyFaceDetector.loadFromUri('/models');
            await faceapi.nets.faceLandmark68Net.loadFromUri('/models');
            await faceapi.nets.faceRecognitionNet.loadFromUri('/models');
            await faceapi.nets.faceExpressionNet.loadFromUri('/models');
        }

        async function detectFaces() {
            const detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks().withFaceExpressions();
            context.clearRect(0, 0, canvas.width, canvas.height);
            canvas.width = video.videoWidth;
            canvas.height = video.videoHeight;
            faceapi.draw.drawDetections(canvas, detections);
            if (detections.length > 0 && !isPlaying) {
                isPlaying = true;
                fetch('http://192.168.40.73:5000/moveF');
            } else if (detections.length === 0 && isPlaying) {
                isPlaying = false;
                fetch('http://192.168.40.73:5000/stop');
            }
        }

        setupCamera().then(video => {
            loadModels();
            video.play();
            setInterval(detectFaces, 100);
        });
    </script>
</body>
</html>
